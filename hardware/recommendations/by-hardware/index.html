<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-hardware/recommendations/by-hardware" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">Selecting AI Hardware | Jan</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://jan.ai/img/jan-social-card.png"><meta data-rh="true" name="twitter:image" content="https://jan.ai/img/jan-social-card.png"><meta data-rh="true" property="og:url" content="https://jan.ai/hardware/recommendations/by-hardware"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Selecting AI Hardware | Jan"><meta data-rh="true" name="description" content="When selecting a GPU for LLMs, remember that it&#x27;s not just about the GPU itself. Consider the synergy with other components in your PC:"><meta data-rh="true" property="og:description" content="When selecting a GPU for LLMs, remember that it&#x27;s not just about the GPU itself. Consider the synergy with other components in your PC:"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://jan.ai/hardware/recommendations/by-hardware"><link data-rh="true" rel="alternate" href="https://jan.ai/hardware/recommendations/by-hardware" hreflang="en"><link data-rh="true" rel="alternate" href="https://jan.ai/hardware/recommendations/by-hardware" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.2ffd5302.css">
<link rel="preload" href="/assets/js/runtime~main.980da2df.js" as="script">
<link rel="preload" href="/assets/js/main.fb7d1652.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Jan Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="Jan Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Jan</b></a><a class="navbar__item navbar__link" href="/events/hcmc-oct23">Company</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_l3_l docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><main class="docMainContainer_gTbr docMainContainerEnhanced_Uz_u"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Selecting AI Hardware</h1></header><p>When selecting a GPU for LLMs, remember that it&#x27;s not just about the GPU itself. Consider the synergy with other components in your PC:</p><ul><li><strong>CPU</strong>: To ensure efficient processing, pair your GPU with a powerful CPU. LLMs benefit from fast processors, so having a capable CPU is essential.</li><li><strong>RAM</strong>: Sufficient RAM is crucial for LLMs. They can be memory-intensive, and having enough RAM ensures smooth operation.</li><li><strong>Cooling System</strong>: LLMs can push your PC&#x27;s hardware to the limit. A robust cooling system helps maintain optimal temperatures, preventing overheating and performance throttling.</li></ul><p>By taking all of these factors into account, you can build a home PC setup that&#x27;s well-equipped to handle the demands of running LLMs effectively and efficiently.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="gpu-selection">GPU Selection<a href="#gpu-selection" class="hash-link" aria-label="Direct link to GPU Selection" title="Direct link to GPU Selection">​</a></h2><p>Selecting the optimal GPU for running Large Language Models (LLMs) on your home PC is a decision influenced by your budget and the specific LLMs you intend to work with. Your choice should strike a balance between performance, efficiency, and cost-effectiveness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gpu-comparison">GPU Comparison<a href="#gpu-comparison" class="hash-link" aria-label="Direct link to GPU Comparison" title="Direct link to GPU Comparison">​</a></h3><table><thead><tr><th>GPU</th><th>Price</th><th>Cores</th><th>VRAM (GB)</th><th>Bandwth (T/s)</th><th>Power</th></tr></thead><tbody><tr><td>Nvidia H100</td><td>40000</td><td>18432</td><td>80</td><td>2</td><td></td></tr><tr><td>Nvidia A100</td><td>15000</td><td>6912</td><td>80</td><td></td><td></td></tr><tr><td>Nvidia A100</td><td>7015</td><td>6912</td><td>40</td><td></td><td></td></tr><tr><td>Nvidia A10</td><td>2799</td><td>9216</td><td>24</td><td></td><td></td></tr><tr><td>Nvidia RTX A6000</td><td>4100</td><td>10752</td><td>48</td><td>0.768</td><td></td></tr><tr><td>Nvidia RTX 6000</td><td>6800</td><td>4608</td><td>46</td><td></td><td></td></tr><tr><td>Nvidia RTX 4090 Ti</td><td>2000</td><td>18176</td><td>24</td><td></td><td></td></tr><tr><td>Nvidia RTX 4090</td><td>1800</td><td>16384</td><td>24</td><td>1.008</td><td></td></tr><tr><td>Nvidia RTX 3090</td><td>1450</td><td>10496</td><td>24</td><td></td><td></td></tr><tr><td>Nvidia RTX 3080</td><td>700</td><td>8704</td><td>12</td><td></td><td></td></tr><tr><td>Nvidia RTX 3070</td><td>900</td><td>6144</td><td>8</td><td></td><td></td></tr><tr><td>Nvidia L4</td><td>2711</td><td>7424</td><td>24</td><td></td><td></td></tr><tr><td>Nvidia T4</td><td>2299</td><td>2560</td><td>16</td><td></td><td></td></tr><tr><td>AMD Radeon RX 6900 XT</td><td>1000</td><td>5120</td><td>16</td><td></td><td></td></tr><tr><td>AMD Radeon RX 6800 XT</td><td>420</td><td>4608</td><td>16</td><td></td><td></td></tr></tbody></table><p>*<!-- -->Market prices as of Oct 2023 via Amazon/PCMag</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="other-considerations">Other Considerations<a href="#other-considerations" class="hash-link" aria-label="Direct link to Other Considerations" title="Direct link to Other Considerations">​</a></h3><p>In general, the following GPU features are important for running LLMs:</p><ul><li><strong>High VRAM:</strong> LLMs are typically very large and complex models, so they require a GPU with a high amount of VRAM. This will allow the model to be loaded into memory and processed efficiently.</li><li><strong>CUDA Compatibility:</strong> When running LLMs on a GPU, CUDA compatibility is paramount. CUDA is NVIDIA&#x27;s parallel computing platform, and it plays a vital role in accelerating deep learning tasks. LLMs, with their extensive matrix calculations, heavily rely on parallel processing. Ensuring your GPU supports CUDA is like having the right tool for the job. It allows the LLM to leverage the GPU&#x27;s parallel processing capabilities, significantly speeding up model training and inference.</li><li><strong>Number of CUDA, Tensor, and RT Cores:</strong> High-performance NVIDIA GPUs have both CUDA and Tensor cores. These cores are responsible for executing the neural network computations that underpin LLMs&#x27; language understanding and generation. The more CUDA cores your GPU has, the better equipped it is to handle the massive computational load that LLMs impose. Tensor cores in your GPU, further enhance LLM performance by accelerating the critical matrix operations integral to language modeling tasks.</li><li><strong>Generation (Series)</strong>: When selecting a GPU for LLMs, consider its generation or series (e.g., RTX 30 series). Newer GPU generations often come with improved architectures and features. For LLM tasks, opting for the latest generation can mean better performance, energy efficiency, and support for emerging AI technologies. Avoid purchasing, RTX-2000 series GPUs which are much outdated nowadays.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="cpu-selection">CPU Selection<a href="#cpu-selection" class="hash-link" aria-label="Direct link to CPU Selection" title="Direct link to CPU Selection">​</a></h2><p>Selecting the right CPU for running Large Language Models (LLMs) on your home PC is contingent on your budget and the specific LLMs you intend to work with. It&#x27;s a decision that warrants careful consideration, as the CPU plays a pivotal role in determining the overall performance of your system.</p><p>In general, the following CPU features are important for running LLMs:</p><ul><li><strong>Number of Cores and Threads:</strong> the number of CPU cores and threads influences parallel processing. More cores and threads help handle the complex computations involved in language models. For tasks like training and inference, a higher core/thread count can significantly improve processing speed and efficiency, enabling quicker results.</li><li><strong>High clock speed:</strong> The base clock speed, or base frequency, represents the CPU&#x27;s default operating speed. So having a CPU with a high clock speed. This will allow the model to process instructions more quickly, which can further improve performance.</li><li><strong>Base Power (TDP):</strong> LLMs often involve long training sessions and demanding computations. Therefore, a lower Thermal Design Power (TDP) is desirable. A CPU with a lower TDP consumes less power and generates less heat during prolonged LLM operations. This not only contributes to energy efficiency but also helps maintain stable temperatures in your system, preventing overheating and potential performance throttling.</li><li><strong>Generation (Series):</strong> Consider its generation or series (e.g., 9th Gen, 11th Gen Intel Core). Newer CPU generations often come with architectural improvements that enhance performance and efficiency. For LLM tasks, opting for a more recent generation can lead to faster and more efficient language model training and inference.</li><li><strong>Support for AVX512:</strong> AVX512 is a set of vector instruction extensions that can be used to accelerate machine learning workloads. Many LLMs are optimized to take advantage of AVX512, so it is important to make sure that your CPU supports this instruction set.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="here-are-some-cpu-options-for-running-llms">Here are some CPU options for running LLMs:<a href="#here-are-some-cpu-options-for-running-llms" class="hash-link" aria-label="Direct link to Here are some CPU options for running LLMs:" title="Direct link to Here are some CPU options for running LLMs:">​</a></h3><ol><li><strong>Intel Core i7-12700K</strong>: Slightly less potent than the Core i9-12900K, the Intel Core i7-12700K is still a powerful CPU. With 12 cores and 20 threads, it strikes a balance between performance and cost-effectiveness. This CPU is well-suited for running mid-sized and large LLMs, making it a compelling option.</li><li><strong>Intel Core i9-12900K</strong>: Positioned as a high-end CPU, the Intel Core i9-12900K packs a formidable punch with its 16 cores and 24 threads. It&#x27;s one of the fastest CPUs available, making it an excellent choice for handling large and intricate LLMs. The abundance of cores and threads translates to exceptional parallel processing capabilities, which is crucial for tasks involving massive language models.</li><li><strong>AMD Ryzen 9 5950X</strong>: Representing AMD&#x27;s high-end CPU offering, the Ryzen 9 5950X boasts 16 cores and 32 threads. While it may not quite match the speed of the Core i9-12900K, it remains a robust and cost-effective choice. Its multicore prowess enables smooth handling of LLM workloads, and its affordability makes it an attractive alternative.</li><li><strong>AMD Ryzen 7 5800X</strong>: Slightly less potent than the Ryzen 9 5950X, the Ryzen 7 5800X is still a formidable CPU with 8 cores and 16 threads. It&#x27;s well-suited for running mid-sized and smaller LLMs, providing a compelling blend of performance and value.</li></ol><p>For those operating within budget constraints, there are more budget-friendly CPU options:</p><ul><li><strong>Intel Core i5-12600K</strong>: The Core i5-12600K is a capable mid-range CPU that can still handle LLMs effectively, though it may not be optimized for the largest or most complex models.</li><li><strong>AMD Ryzen 5 5600X</strong>: The Ryzen 5 5600X offers a balance of performance and affordability. It&#x27;s suitable for running smaller to mid-sized LLMs without breaking the bank.</li></ul><p><strong>When selecting a CPU for LLMs, consider the synergy with other components in your PC:</strong></p><ul><li><strong>GPU</strong>: Pair your CPU with a powerful GPU to ensure smooth processing of LLMs. Some language models, particularly those used for AI, rely on GPU acceleration for optimal performance.</li><li><strong>RAM</strong>: Adequate RAM is essential for LLMs, as these models can be memory-intensive. Having enough RAM ensures that your CPU can operate efficiently without bottlenecks.</li><li><strong>Cooling System</strong>: Given the resource-intensive nature of LLMs, a robust cooling system is crucial to maintain optimal temperatures and prevent performance throttling.</li></ul><p>By carefully weighing your budget and performance requirements and considering the interplay of components in your PC, you can assemble a well-rounded system that&#x27;s up to the task of running LLMs efficiently.</p><blockquote><p>📝 <strong>Note:</strong> It is important to note that these are just general recommendations. The specific CPU requirements for your LLM will vary depending on the specific model you are using and the tasks that you want to perform with it. If you are unsure what CPU to get, it is best to consult with an expert.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ram-selection">RAM Selection<a href="#ram-selection" class="hash-link" aria-label="Direct link to RAM Selection" title="Direct link to RAM Selection">​</a></h2><p>The amount of RAM you need to run an LLM depends on the size and complexity of the model, as well as the tasks you want to perform with it. For example, if you are simply running inference on a pre-trained LLM, you may be able to get away with using a relatively modest amount of RAM. However, if you are training a new LLM from scratch, or if you are running complex tasks like fine-tuning or code generation, you will need more RAM.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="here-is-a-general-guide-to-ram-selection-for-running-llms">Here is a general guide to RAM selection for running LLMs:<a href="#here-is-a-general-guide-to-ram-selection-for-running-llms" class="hash-link" aria-label="Direct link to Here is a general guide to RAM selection for running LLMs:" title="Direct link to Here is a general guide to RAM selection for running LLMs:">​</a></h3><ul><li><strong>Capacity:</strong> The amount of RAM you need will depend on the size and complexity of the LLM model you want to run. For inference, you will need at least 16GB of RAM, but 32GB or more is ideal for larger models and more complex tasks. For training, you will need at least 64GB of RAM, but 128GB or more is ideal for larger models and more complex tasks.</li><li><strong>Speed:</strong> LLMs can benefit from having fast RAM, so it is recommended to use DDR4 or DDR5 RAM with a speed of at least 3200MHz.</li><li><strong>Latency:</strong> RAM latency is the amount of time it takes for the CPU to access data in memory. Lower latency is better for performance, so it is recommended to look for RAM with a low latency rating.</li><li><strong>Timing:</strong> RAM timing is a set of parameters that control how the RAM operates. It is important to make sure that the RAM timing is compatible with your motherboard and CPU.</li></ul><p>R<strong>ecommended RAM</strong> <strong>options for running LLMs:</strong></p><ul><li><strong>Inference:</strong> For inference on pre-trained LLMs, you will need at least 16GB of RAM. However, 32GB or more is ideal for larger models and more complex tasks.</li><li><strong>Training:</strong> For training LLMs from scratch, you will need at least 64GB of RAM. However, 128GB or more is ideal for larger models and more complex tasks.</li></ul><p>In addition to the amount of RAM, it is also important to consider the speed of the RAM. LLMs can benefit from having fast RAM, so it is recommended to use DDR4 or DDR5 RAM with a speed of at least 3200MHz.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="motherboard-selection">Motherboard Selection<a href="#motherboard-selection" class="hash-link" aria-label="Direct link to Motherboard Selection" title="Direct link to Motherboard Selection">​</a></h2><p>When picking a motherboard to run advanced language models, you need to think about a few things. First, consider the specific language model you want to use, the type of CPU and GPU in your computer, and your budget. Here are some suggestions:</p><ol><li><strong>ASUS ROG Maximus Z790 Hero:</strong> This is a top-notch motherboard with lots of great features. It works well with Intel&#x27;s latest CPUs, fast DDR5 memory, and PCIe 5.0 devices. It&#x27;s also good at keeping things cool, which is important for running demanding language models.</li><li><strong>MSI MEG Z790 Ace:</strong> Similar to the ASUS ROG Maximus, this motherboard is high-end and has similar features. It&#x27;s good for running language models too.</li><li><strong>Gigabyte Z790 Aorus Master:</strong> This one is more budget-friendly but still works great with Intel&#x27;s latest CPUs, DDR5 memory, and fast PCIe 5.0 devices. It&#x27;s got a strong power system, which helps with running language models.</li></ol><p>If you&#x27;re on a tighter budget, you might want to check out mid-range options like the <strong>ASUS TUF Gaming Z790-Plus WiFi</strong> or the <strong>MSI MPG Z790 Edge WiFi DDR5</strong>. They offer good performance without breaking the bank.</p><p>No matter which motherboard you pick, make sure it works with your CPU and GPU. Also, check that it has the features you need, like enough slots for your GPU and storage drives.</p><p>Other things to think about when choosing a motherboard for language models:</p><ul><li><strong>Cooling:</strong> Language models can make your CPU work hard, so a motherboard with good cooling is a must. This keeps your CPU from getting too hot.</li><li><strong>Memory:</strong> Language models need lots of memory, so make sure your motherboard supports a good amount of it. Check if it works with the type of memory you want to use, like DDR5 or DDR4.</li><li><strong>Storage:</strong> Language models can create and store a ton of data. So, look for a motherboard with enough slots for your storage drives.</li><li><strong>BIOS:</strong> The BIOS controls your motherboard. Make sure it&#x27;s up-to-date and has the latest features, especially if you plan to overclock or undervolt your system.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="cooling-system-selection">Cooling System Selection<a href="#cooling-system-selection" class="hash-link" aria-label="Direct link to Cooling System Selection" title="Direct link to Cooling System Selection">​</a></h2><p>Modern computers have two critical components, the CPU and GPU, which can heat up during high-performance tasks. To prevent overheating, they come with built-in temperature controls that automatically reduce performance when temperatures rise. To keep them cool and maintain optimal performance, you need a reliable cooling system.</p><p>For laptops, the only choice is a fan-based cooling system. Laptops have built-in fans and copper pipes to dissipate heat. Many gaming laptops even have two separate fans: one for the CPU and another for the GPU.</p><p>For desktop computers, you have the option to install more efficient water cooling systems. These are highly effective but can be expensive. Or you can install more cooling fans to keep you components cool.</p><p>Keep in mind that dust can accumulate in fan-based cooling systems, leading to malfunctions. So periodically clean the dust to keep your cooling system running smoothly.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="use-macbook-to-run-llms">Use MacBook to run LLMs<a href="#use-macbook-to-run-llms" class="hash-link" aria-label="Direct link to Use MacBook to run LLMs" title="Direct link to Use MacBook to run LLMs">​</a></h2><p>An Apple MacBook equipped with either the M1 or the newer M2 Pro/Max processor. These cutting-edge chips leverage Apple&#x27;s innovative Unified Memory Architecture (UMA), which revolutionizes the way the CPU and GPU interact with memory resources. This advancement plays a pivotal role in enhancing the performance and capabilities of LLMs.</p><p>Unified Memory Architecture, as implemented in Apple&#x27;s M1 and M2 series processors, facilitates seamless and efficient data access for both the CPU and GPU. Unlike traditional systems where data needs to be shuttled between various memory pools, UMA offers a unified and expansive memory pool that can be accessed by both processing units without unnecessary data transfers. This transformative approach significantly minimizes latency while concurrently boosting data access bandwidth, resulting in substantial improvements in both the speed and quality of outputs.
<img loading="lazy" src="https://media.discordapp.net/attachments/1148534242104574012/1156600109967089714/IMG_3722.webp?ex=6516380a&amp;is=6514e68a&amp;hm=ebe3b6ecb1edb44cde58bd8d3fdd46cef66b60aa41ea6c03b51325fa65f8517e&amp;=&amp;width=807&amp;height=426" alt="UMA" class="img_ev3q"></p><p>The M1 and M2 Pro/Max chips offer varying levels of unified memory bandwidth, further underscoring their prowess in handling data-intensive tasks like AI processing. The M1/M2 Pro chip boasts an impressive capacity of up to 200 GB/s of unified memory bandwidth, while the M1/M2 Max takes it a step further, supporting up to a staggering 400 GB/s of unified memory bandwidth. This means that regardless of the complexity and demands of the AI tasks at hand, these Apple laptops armed with M1 or M2 processors are well-equipped to handle them with unparalleled efficiency and speed.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="calculating-vram-requirements-for-an-llm">Calculating vRAM Requirements for an LLM<a href="#calculating-vram-requirements-for-an-llm" class="hash-link" aria-label="Direct link to Calculating vRAM Requirements for an LLM" title="Direct link to Calculating vRAM Requirements for an LLM">​</a></h2><p><strong>For example:</strong> Calculating the VRAM required to run a 13-billion-parameter Large Language Model (LLM) involves considering the model size, batch size, sequence length, token size, and any additional overhead. Here&#x27;s how you can estimate the VRAM required for a 13B LLM:</p><ol><li><strong>Model Size</strong>: Find out the size of the 13B LLM in terms of the number of parameters. This information is typically provided in the model&#x27;s documentation. A 13-billion-parameter model has 13,000,000,000 parameters.</li><li><strong>Batch Size</strong>: Decide on the batch size you want to use during inference. The batch size represents how many input samples you process simultaneously. Smaller batch sizes require less VRAM.</li><li><strong>Sequence Length</strong>: Determine the average length of the input text sequences you&#x27;ll be working with. Sequence length can impact VRAM requirements; longer sequences need more memory.</li><li><strong>Token Size</strong>: Understand the memory required to store one token in bytes. Most LLMs use 4 bytes per token.</li><li><strong>Overhead</strong>: Consider any additional memory overhead for intermediate computations and framework requirements. Overhead can vary but should be estimated based on your specific setup.</li></ol><p>Use the following formula to estimate the VRAM required:</p><p><strong>VRAM Required (in gigabytes)</strong> = <code>Model Parameters x Token Size x Batch Size x Sequence Length + Overhead</code></p><ul><li><strong>Model Parameters</strong>: 13,000,000,000 parameters for a 13B LLM.</li><li><strong>Token Size</strong>: Usually 4 bytes per token.</li><li><strong>Batch Size</strong>: Choose your batch size.</li><li><strong>Sequence Length</strong>: The average length of input sequences.</li><li><strong>Overhead</strong>: Any additional VRAM required based on your setup.</li></ul><p>Here&#x27;s an example:</p><p>Suppose you want to run a 13B LLM with the following parameters:</p><ul><li><strong>Batch Size</strong>: 4</li><li><strong>Sequence Length</strong>: 512 tokens</li><li><strong>Token Size</strong>: 4 bytes</li><li><strong>Estimated Overhead</strong>: 2 GB</li></ul><p>VRAM Required (in gigabytes) = <code>(13,000,000,000 x 4 x 4 x 512) + 2</code></p><p>VRAM Required (in gigabytes) = <code>(8,388,608,000) + 2,000</code></p><p>VRAM Required (in gigabytes) ≈ <code>8,390,608,000 bytes</code></p><p>To convert this to gigabytes, divide by <code>1,073,741,824 (1 GB)</code></p><p>VRAM Required (in gigabytes) ≈ <code>8,390,608,000 / 1,073,741,824 ≈ 7.8 GB</code></p><p>So, to run a 13-billion-parameter LLM with the specified parameters and overhead, you would need approximately 7.8 gigabytes of VRAM on your GPU. Make sure to have some additional VRAM for stable operation and consider testing the setup in practice to monitor VRAM usage accurately.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/janhq/jan/tree/main/docs/docs/hardware/recommendations/by-hardware.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-10-24T02:42:47.000Z">Oct 24, 2023</time></b> by <b>0xSage</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#gpu-selection" class="table-of-contents__link toc-highlight">GPU Selection</a><ul><li><a href="#gpu-comparison" class="table-of-contents__link toc-highlight">GPU Comparison</a></li><li><a href="#other-considerations" class="table-of-contents__link toc-highlight">Other Considerations</a></li></ul></li><li><a href="#cpu-selection" class="table-of-contents__link toc-highlight">CPU Selection</a><ul><li><a href="#here-are-some-cpu-options-for-running-llms" class="table-of-contents__link toc-highlight">Here are some CPU options for running LLMs:</a></li></ul></li><li><a href="#ram-selection" class="table-of-contents__link toc-highlight">RAM Selection</a><ul><li><a href="#here-is-a-general-guide-to-ram-selection-for-running-llms" class="table-of-contents__link toc-highlight">Here is a general guide to RAM selection for running LLMs:</a></li></ul></li><li><a href="#motherboard-selection" class="table-of-contents__link toc-highlight">Motherboard Selection</a></li><li><a href="#cooling-system-selection" class="table-of-contents__link toc-highlight">Cooling System Selection</a></li><li><a href="#use-macbook-to-run-llms" class="table-of-contents__link toc-highlight">Use MacBook to run LLMs</a></li><li><a href="#calculating-vram-requirements-for-an-llm" class="table-of-contents__link toc-highlight">Calculating vRAM Requirements for an LLM</a></li></ul></div></div></div></div></main></div></div><footer class="flex-shrink-0 border-t dark:border-gray-800 border-gray-200 py-10"><div class="container"><div class="grid grid-cols-2 gap-8 md:grid-cols-2 lg:grid-cols-5"><div class="col-span-2 lg:col-span-1"><h6 class="mb-3">Jan</h6><p class="dark:text-gray-400 text-gray-600">Run Large Language Models locally on Windows, Mac and Linux. Available on Desktop and Cloud-Native.</p></div><div class="lg:text-right"><h6 class="mb-3">Resources</h6><ul><li><a href="/" target="_self" class="inline-block py-1 dark:text-gray-400 text-gray-600">Home</a></li><li><a href="/platform" target="_self" class="inline-block py-1 dark:text-gray-400 text-gray-600">Platform</a></li><li><a href="/solutions" target="_self" class="inline-block py-1 dark:text-gray-400 text-gray-600">Solutions</a></li></ul></div><div class="lg:text-right"><h6 class="mb-3">For Developers</h6><ul><li><a href="/docs" target="_self" class="inline-block py-1 dark:text-gray-400 text-gray-600">Documentation (WIP)</a></li><li><a href="/hardware" target="_self" class="inline-block py-1 dark:text-gray-400 text-gray-600">Hardware (WIP)</a></li><li><a href="/api" target="_self" class="inline-block py-1 dark:text-gray-400 text-gray-600">API (WIP)</a></li><li><a href="https://github.com/janhq/jan/releases" target="_blank" class="inline-block py-1 dark:text-gray-400 text-gray-600">Changelog</a></li></ul></div><div class="lg:text-right"><h6 class="mb-3">Community</h6><ul><li><a href="https://github.com/janhq/jan" target="_blank" class="inline-block py-1 dark:text-gray-400 text-gray-600">Github</a></li><li><a href="https://discord.gg/FTk2MvZwJH" target="_blank" class="inline-block py-1 dark:text-gray-400 text-gray-600">Discord</a></li><li><a href="https://twitter.com/janhq_" target="_blank" class="inline-block py-1 dark:text-gray-400 text-gray-600">Twitter</a></li></ul></div><div class="lg:text-right"><h6 class="mb-3">Company</h6><ul><li><a href="/about" target="_self" class="inline-block py-1 dark:text-gray-400 text-gray-600">About</a></li><li><a href="https://janai.bamboohr.com/careers" target="_blank" class="inline-block py-1 dark:text-gray-400 text-gray-600">Careers</a></li></ul></div></div></div><div class="container mt-8"><span class="dark:text-gray-300 text-gray-700">©<!-- -->2023<!-- --> Jan AI Pte Ltd.</span></div></footer></div>
<script src="/assets/js/runtime~main.980da2df.js"></script>
<script src="/assets/js/main.fb7d1652.js"></script>
</body>
</html>